---
title: "Processing Long Audio Files with VAD"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Processing Long Audio Files with VAD}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Why Use VAD?
  
Voice Activity Detection (VAD) automatically splits long audio files at natural
pauses in speech, providing several benefits:

- **Better quality**: Segments are created at natural pauses, not mid-word
- **Universal compatibility**: Works with ANY model (not just those with timestamps)
- **Reduced memory usage**: Processes audio in segments rather than all at once
- **Timing information**: Provides start time and duration for each segment

## Basic Usage

```{r basic}
library(sherpa.onnx)

# Create a recognizer with any model
rec <- OfflineRecognizer$new(model = "whisper-tiny")

# Transcribe a long podcast with VAD enabled
result <- rec$transcribe("podcast.wav", use_vad = TRUE)

# Full text
print(result$text)

# Individual segments with timing
for (i in seq_along(result$segments)) {
  cat(sprintf("[%.1f - %.1f]: %s\n",
              result$segment_starts[i],
              result$segment_starts[i] + result$segment_durations[i],
              result$segments[i]))
}
```

## Understanding VAD Output

When using VAD, the transcription result includes additional fields:

- `text`: The full transcribed text (all segments joined)
- `segments`: Character vector of individual segment texts
- `segment_starts`: Start time in seconds for each segment
- `segment_durations`: Duration in seconds for each segment
- `num_segments`: Total number of speech segments detected

```{r output}
result <- rec$transcribe("interview.wav", use_vad = TRUE)

# View summary with VAD statistics
summary(result)
# Shows: segments count, total duration, speech duration, speech ratio
```

## Tuning VAD Parameters

The VAD behavior can be tuned with several parameters:

### vad_threshold (default: 0.5)

Speech detection sensitivity. Range: 0-1.

- Lower values = more sensitive (detects quieter speech)
- Higher values = less sensitive (may miss quiet passages)

```{r threshold}
# More sensitive - for quiet speakers
result <- rec$transcribe("quiet_audio.wav",
                        use_vad = TRUE,
                        vad_threshold = 0.3)

# Less sensitive - for noisy environments
result <- rec$transcribe("noisy_audio.wav",
                        use_vad = TRUE,
                        vad_threshold = 0.7)
```

### vad_min_silence (default: 0.5 seconds)

Minimum silence duration to trigger a segment split.

- Shorter = more segments (splits on brief pauses)
- Longer = fewer segments (only splits on longer pauses)

```{r silence}
# More aggressive splitting for fast speakers
result <- rec$transcribe("fast_speech.wav",
                        use_vad = TRUE,
                        vad_min_silence = 0.3)

# Less splitting for natural conversation
result <- rec$transcribe("conversation.wav",
                        use_vad = TRUE,
                        vad_min_silence = 0.8)
```

### vad_min_speech (default: 0.25 seconds)

Minimum speech duration to keep a segment. Segments shorter than this are discarded.

```{r min_speech}
# Filter out very short utterances (like "um", "uh")
result <- rec$transcribe("meeting.wav",
                        use_vad = TRUE,
                        vad_min_speech = 0.5)
```

### vad_max_speech (default: 30 seconds)

Maximum speech duration before forcing a segment split. Useful to prevent memory
issues with very long continuous speech.

```{r max_speech}
# Force splits for a lecture with few natural pauses
result <- rec$transcribe("lecture.wav",
                        use_vad = TRUE,
                        vad_max_speech = 60.0)
```

## When to Use VAD

**Recommended for:**

- Audio longer than ~30 seconds
- Interviews or conversations
- Podcasts and lectures
- Audio with natural pauses

**May not be needed for:**

- Short audio clips (< 30 seconds)
- Continuous speech without pauses
- When you need token-level timestamps (use model timestamps instead)

## Example: Processing a Meeting Recording

```{r meeting}
library(sherpa.onnx)

rec <- OfflineRecognizer$new(model = "parakeet-v3")

# Process a 1-hour meeting recording
result <- rec$transcribe(
  "meeting_recording.wav",
  use_vad = TRUE,
  vad_min_silence = 0.6,  # Split on pauses > 0.6 seconds
  vad_min_speech = 0.3,   # Keep segments > 0.3 seconds
  verbose = TRUE          # Show progress
)

# Get summary statistics
summary(result)

# Create a simple transcript with timestamps
transcript <- data.frame(
  start = result$segment_starts,
  end = result$segment_starts + result$segment_durations,
  text = result$segments
)

# Format timestamps as MM:SS
transcript$timestamp <- sprintf("%02d:%02d",
                                floor(transcript$start / 60),
                                floor(transcript$start %% 60))

print(transcript[, c("timestamp", "text")])
```

## Performance Tips

1. **Use `verbose = FALSE`** in production to skip progress messages
2. **Adjust `vad_max_speech`** for memory-constrained environments
3. **The Silero VAD model** is automatically downloaded and cached on first use
4. **VAD processing** adds minimal overhead compared to transcription time
