---
title: "Processing Long Audio Files"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Processing Long Audio Files}

  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  purl = FALSE
)
```
## Automatic VAD for Long Audio

When using Whisper models, Voice Activity Detection (VAD) is automatically
enabled for audio longer than 29 seconds. This happens transparently - you
don't need to configure anything:

```{r basic}
library(sherpa.onnx)

# Create a recognizer with Whisper
rec <- OfflineRecognizer$new(model = "whisper-tiny")

# Long audio automatically uses VAD
result <- rec$transcribe("podcast.wav")

# Full text
print(result$text)

# For long audio, segment information is available
if (!is.null(result$num_segments)) {
  cat(sprintf("Transcribed in %d segments\n", result$num_segments))

  # Individual segments with timing
  for (i in seq_along(result$segments)) {
    cat(sprintf("[%.1f - %.1f]: %s\n",
                result$segment_starts[i],
                result$segment_starts[i] + result$segment_durations[i],
                result$segments[i]))
  }
}
```

## Why Automatic VAD?

Whisper models have a 30-second context window limit. For longer audio,
the package automatically:

1. Detects speech segments using the Silero VAD model
2. Groups segments into batches up to 29 seconds
3. Transcribes each batch
4. Combines the results seamlessly

This provides:

- **Better quality**: Segments are created at natural pauses, not mid-word
- **Reduced memory usage**: Processes audio in manageable chunks
- **Timing information**: Provides start time and duration for each segment

## Understanding the Output

For short audio (< 29 seconds), you get standard transcription fields:

- `text`: The transcribed text
- `tokens`: Individual tokens
- `timestamps`: Token timestamps (if supported)

For long audio with VAD, additional fields are available:

- `segments`: Character vector of individual segment texts
- `segment_starts`: Start time in seconds for each segment
- `segment_durations`: Duration in seconds for each segment
- `num_segments`: Total number of batched segments

```{r output}
result <- rec$transcribe("long_interview.wav")

# View summary with VAD statistics
summary(result)
# Shows: segments count, total duration, speech duration, speech ratio
```

## Standalone VAD Function

For fine-grained control over voice activity detection, use the standalone
`vad()` function:

```{r vad-standalone}
# Detect speech segments without transcription
segments <- vad("recording.wav")

# View detected segments
print(segments)

# Get detailed statistics
summary(segments)

# Convert to data frame
df <- as.data.frame(segments)
print(df)
```

### Tuning VAD Parameters

The `vad()` function accepts parameters to tune detection:

```{r vad-params}
# More sensitive detection (for quiet audio)
segments <- vad("quiet_audio.wav", threshold = 0.3)

# Less sensitive (for noisy environments)
segments <- vad("noisy_audio.wav", threshold = 0.7)

# Shorter minimum silence (more segments)
segments <- vad("fast_speech.wav", min_silence = 0.3)

# Longer minimum silence (fewer segments)
segments <- vad("conversation.wav", min_silence = 0.8)

# Filter out very short utterances
segments <- vad("meeting.wav", min_speech = 0.5)
```

### Manual Transcription of VAD Segments

You can manually transcribe VAD segments for full control:
```{r manual-vad}
library(sherpa.onnx)

# Create recognizer
rec <- OfflineRecognizer$new(model = "whisper-tiny")

# Get VAD segments
segments <- vad("long_recording.wav", verbose = FALSE)

# Transcribe each segment individually
# (useful if you need per-segment control)
for (i in seq_len(segments$num_segments)) {
  seg <- segments$segments[[i]]
  cat(sprintf("\nSegment %d (%.1f - %.1f sec):\n",
              i, seg$start_time, seg$start_time + seg$duration))

  # You could transcribe individual segments here
  # using transcribe_samples_() if needed
}
```

## Other Model Types

For non-Whisper models (Parakeet, SenseVoice, etc.), VAD is not automatically
enabled because these models can handle longer audio natively. The entire
audio is transcribed at once:

```{r other-models}
# Parakeet can handle long audio without VAD
rec <- OfflineRecognizer$new(model = "parakeet-v3")
result <- rec$transcribe("long_audio.wav")  # No VAD, direct transcription
```

## Performance Tips

1. **Whisper + long audio**: VAD is automatic, no configuration needed
2. **Short audio**: VAD is skipped automatically (no overhead)
3. **Non-Whisper models**: Use as-is for long audio
4. **Custom VAD needs**: Use the standalone `vad()` function
5. **The Silero VAD model** is automatically downloaded and cached on first use
