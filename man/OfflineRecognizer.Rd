% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/recognizer.R
\name{OfflineRecognizer}
\alias{OfflineRecognizer}
\title{Offline Speech Recognizer}
\description{
R6 class for offline speech recognition using sherpa-onnx.
Supports multiple model architectures including Whisper, Paraformer,
SenseVoice, and Transducer models.
}
\examples{

## ------------------------------------------------
## Method `OfflineRecognizer$new`
## ------------------------------------------------

\dontrun{
# Create recognizer with shorthand (auto-detects threads)
rec <- OfflineRecognizer$new(model = "whisper-tiny")

# Create recognizer with specific thread count
rec <- OfflineRecognizer$new(model = "whisper-tiny", num_threads = 4)

# Create recognizer with HuggingFace repo
rec <- OfflineRecognizer$new(
  model = "csukuangfj/sherpa-onnx-whisper-tiny.en"
)

# Create recognizer with local model
rec <- OfflineRecognizer$new(model = "/path/to/model")
}

## ------------------------------------------------
## Method `OfflineRecognizer$transcribe`
## ------------------------------------------------

\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
result <- rec$transcribe("audio.wav")

# Print with custom format
print(result)

# Access fields
cat("Transcription:", result$text, "\n")

# Extract text
text <- as.character(result)

# Detailed information
summary(result)
}

## ------------------------------------------------
## Method `OfflineRecognizer$transcribe_batch`
## ------------------------------------------------

\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
results <- rec$transcribe_batch(c("file1.wav", "file2.wav"))

# Access results via tibble columns
print(results$text)
print(results$file[1])

# Access list-columns
first_tokens <- results$tokens[[1]]
}

## ------------------------------------------------
## Method `OfflineRecognizer$model_info`
## ------------------------------------------------

\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
info <- rec$model_info()
print(info)
}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-OfflineRecognizer-new}{\code{OfflineRecognizer$new()}}
\item \href{#method-OfflineRecognizer-transcribe}{\code{OfflineRecognizer$transcribe()}}
\item \href{#method-OfflineRecognizer-transcribe_batch}{\code{OfflineRecognizer$transcribe_batch()}}
\item \href{#method-OfflineRecognizer-model_info}{\code{OfflineRecognizer$model_info()}}
\item \href{#method-OfflineRecognizer-clone}{\code{OfflineRecognizer$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OfflineRecognizer-new"></a>}}
\if{latex}{\out{\hypertarget{method-OfflineRecognizer-new}{}}}
\subsection{Method \code{new()}}{
Create a new offline recognizer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OfflineRecognizer$new(
  model = "parakeet-v3",
  language = "auto",
  num_threads = NULL,
  provider = "cpu",
  verbose = TRUE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{model}}{Model specification. Can be:
- A shorthand string: "parakeet-v3", "whisper-tiny", "whisper-base", "sense-voice"
- A HuggingFace repository: "csukuangfj/sherpa-onnx-whisper-tiny.en"
- A local directory path containing model files}

\item{\code{language}}{Language code for multilingual models (default: "auto").
Used for Whisper and SenseVoice models.}

\item{\code{num_threads}}{Number of threads for inference (default: NULL = auto-detect).
If NULL, uses parallel::detectCores() with a maximum of 4 threads.}

\item{\code{provider}}{Execution provider: "cpu", "cuda", or "coreml" (default: "cpu")}

\item{\code{verbose}}{Logical, whether to print status messages (default: TRUE)}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new OfflineRecognizer object
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
# Create recognizer with shorthand (auto-detects threads)
rec <- OfflineRecognizer$new(model = "whisper-tiny")

# Create recognizer with specific thread count
rec <- OfflineRecognizer$new(model = "whisper-tiny", num_threads = 4)

# Create recognizer with HuggingFace repo
rec <- OfflineRecognizer$new(
  model = "csukuangfj/sherpa-onnx-whisper-tiny.en"
)

# Create recognizer with local model
rec <- OfflineRecognizer$new(model = "/path/to/model")
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OfflineRecognizer-transcribe"></a>}}
\if{latex}{\out{\hypertarget{method-OfflineRecognizer-transcribe}{}}}
\subsection{Method \code{transcribe()}}{
Transcribe a WAV file
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OfflineRecognizer$transcribe(wav_path, verbose = TRUE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{wav_path}}{Path to WAV file (must be 16kHz, 16-bit, mono)}

\item{\code{verbose}}{Logical. Show progress messages. Default: TRUE}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Whisper models have a 30-second context window limit. For audio longer than
29 seconds, this method automatically uses Voice Activity Detection (VAD)
to split the audio at natural speech boundaries, transcribes each segment,
and combines the results. This happens transparently - you don't need to
configure anything.

For other model types (Parakeet, SenseVoice, etc.), the entire audio is
transcribed at once regardless of length.

If you need fine-grained control over VAD parameters, use the standalone
`vad()` function to detect speech segments, then transcribe them individually.
}

\subsection{Returns}{
A sherpa_transcription object (list-like) containing:
  - text: Transcribed text
  - tokens: Character vector of tokens
  - timestamps: Numeric vector of timestamps (if supported by model)
  - durations: Numeric vector of token durations (if supported by model)
  - language: Detected language (if supported by model)
  - emotion: Detected emotion (if supported by model)
  - event: Detected audio event (if supported by model)
  - json: Full result as JSON string

  For Whisper models with audio longer than 29 seconds, Voice Activity
  Detection (VAD) is automatically used to segment the audio. In this case,
  additional fields are available:
  - segments: Character vector of segment texts
  - segment_starts: Start times of segments in seconds
  - segment_durations: Duration of segments in seconds
  - num_segments: Number of segments

  The result has a custom print method but maintains list-like access
  (e.g., `result$text`). Use `as.character(result)` to extract just the
  text, or `summary(result)` for detailed statistics.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
result <- rec$transcribe("audio.wav")

# Print with custom format
print(result)

# Access fields
cat("Transcription:", result$text, "\n")

# Extract text
text <- as.character(result)

# Detailed information
summary(result)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OfflineRecognizer-transcribe_batch"></a>}}
\if{latex}{\out{\hypertarget{method-OfflineRecognizer-transcribe_batch}{}}}
\subsection{Method \code{transcribe_batch()}}{
Transcribe multiple WAV files in batch
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OfflineRecognizer$transcribe_batch(wav_paths)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{wav_paths}}{Character vector of WAV file paths}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Tibble with one row per file and columns:
  - file: Input file path (character)
  - text: Transcribed text (character)
  - tokens: List-column of token character vectors
  - timestamps: List-column of timestamp numeric vectors (or NULL)
  - durations: List-column of duration numeric vectors (or NULL)
  - language: Detected language (character, NA if not available)
  - emotion: Detected emotion (character, NA if not available)
  - event: Detected audio event (character, NA if not available)
  - json: Full result as JSON string (character)
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
results <- rec$transcribe_batch(c("file1.wav", "file2.wav"))

# Access results via tibble columns
print(results$text)
print(results$file[1])

# Access list-columns
first_tokens <- results$tokens[[1]]
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OfflineRecognizer-model_info"></a>}}
\if{latex}{\out{\hypertarget{method-OfflineRecognizer-model_info}{}}}
\subsection{Method \code{model_info()}}{
Get model information
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OfflineRecognizer$model_info()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
List with model metadata including:
  - type: "local" or "huggingface"
  - path: Local path to model files
  - model_type: Type of model (whisper, paraformer, sense-voice, transducer)
  - repo: HuggingFace repository (if applicable)
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
rec <- OfflineRecognizer$new(model = "whisper-tiny")
info <- rec$model_info()
print(info)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OfflineRecognizer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-OfflineRecognizer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OfflineRecognizer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
